zabbix_export:
  version: '5.2'
  date: '2020-11-19T14:36:53Z'
  groups:
    -
      name: Templates/Applications
  templates:
    -
      template: 'Hadoop by HTTP'
      name: 'Hadoop by HTTP'
      description: |
        The template gets the Hadoop metrics from cluster's hosts (ResourceManager, NodeManagers, NameNode, DataNodes) by HTTP agent. You should define the IP address (or FQDN) and Web-UI port for the ResourceManager in {$HADOOP.RESOURCEMANAGER.HOST} and {$HADOOP.RESOURCEMANAGER.PORT} macros and for the NameNode in {$HADOOP.NAMENODE.HOST} and {$HADOOP.NAMENODE.PORT} macros respectively. Macros can be set in the template or overridden at the host level.
        
        You can discuss this template or leave feedback on our forum https://www.zabbix.com/forum/zabbix-suggestions-and-feedback/413459-discussion-thread-for-official-zabbix-template-hadoop
        
        Template tooling version used: 0.38
      groups:
        -
          name: Templates/Applications
      applications:
        -
          name: Hadoop
        -
          name: 'Zabbix raw items'
      items:
        -
          name: 'Get DataNodes states'
          type: HTTP_AGENT
          key: hadoop.datanodes.get
          history: 0h
          trends: '0'
          value_type: TEXT
          applications:
            -
              name: 'Zabbix raw items'
          preprocessing:
            -
              type: JAVASCRIPT
              parameters:
                - |
                  try {
                    parsed = JSON.parse(value);
                    var result = [];
                  
                    function getNodes(nodes, state) {
                        Object.keys(nodes).forEach(function (field) {
                            var Node = {};
                            Node['HostName'] = field || '';
                            Node['adminState'] = nodes[field].adminState || '';
                            Node['operState'] = state || '';
                            Node['version'] = nodes[field].version || '';
                            result.push(Node);
                        });
                    }
                  
                    getNodes(JSON.parse(parsed.beans[0].LiveNodes), 'Live');
                    getNodes(JSON.parse(parsed.beans[0].DeadNodes), 'Dead');
                    getNodes(JSON.parse(parsed.beans[0].DecomNodes), 'Decommission');
                    getNodes(JSON.parse(parsed.beans[0].EnteringMaintenanceNodes), 'Maintenance');
                  
                    return JSON.stringify(result);
                  }
                  catch (error) {
                    throw 'Failed to process response received from Hadoop';
                  }
                  
          url: '{$HADOOP.NAMENODE.HOST}:{$HADOOP.NAMENODE.PORT}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo'
        -
          name: 'NameNode: Total blocks'
          type: DEPENDENT
          key: hadoop.namenode.blocks_total
          delay: '0'
          history: 7d
          description: 'Count of blocks tracked by NameNode.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].BlocksTotal.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Blocks allocable'
          type: DEPENDENT
          key: hadoop.namenode.block_capacity
          delay: '0'
          history: 7d
          description: 'Maximum number of blocks allocable.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].BlockCapacity.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Capacity remaining'
          type: DEPENDENT
          key: hadoop.namenode.capacity_remaining
          delay: '0'
          history: 7d
          units: B
          description: 'Available capacity.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].CapacityRemaining.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Corrupt blocks'
          type: DEPENDENT
          key: hadoop.namenode.corrupt_blocks
          delay: '0'
          history: 7d
          description: 'Number of corrupt blocks.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].CorruptBlocks.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Total files'
          type: DEPENDENT
          key: hadoop.namenode.files_total
          delay: '0'
          history: 7d
          description: 'Total count of files tracked by the NameNode.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].FilesTotal.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'Get NameNode stats'
          type: HTTP_AGENT
          key: hadoop.namenode.get
          history: 0h
          trends: '0'
          value_type: TEXT
          applications:
            -
              name: 'Zabbix raw items'
          url: '{$HADOOP.NAMENODE.HOST}:{$HADOOP.NAMENODE.PORT}/jmx'
        -
          name: 'NameNode: Missing blocks'
          type: DEPENDENT
          key: hadoop.namenode.missing_blocks
          delay: '0'
          history: 7d
          description: 'Number of missing blocks.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].MissingBlocks.first()'
          master_item:
            key: hadoop.namenode.get
          triggers:
            -
              expression: '{min(15m)}>0'
              name: 'NameNode: Cluster has missing blocks'
              priority: AVERAGE
              description: 'A missing block is far worse than a corrupt block, because a missing block cannot be recovered by copying a replica.'
        -
          name: 'NameNode: Dead DataNodes'
          type: DEPENDENT
          key: hadoop.namenode.num_dead_data_nodes
          delay: '0'
          history: 7d
          description: 'Count of dead DataNodes.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].NumDeadDataNodes.first()'
            -
              type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 6h
          master_item:
            key: hadoop.namenode.get
          triggers:
            -
              expression: '{min(5m)}>0'
              name: 'NameNode: Cluster has DataNodes in Dead state'
              priority: AVERAGE
              description: 'The death of a DataNode causes a flurry of network activity, as the NameNode initiates replication of blocks lost on the dead nodes.'
        -
          name: 'NameNode: Alive DataNodes'
          type: DEPENDENT
          key: hadoop.namenode.num_live_data_nodes
          delay: '0'
          history: 7d
          description: 'Count of alive DataNodes.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].NumLiveDataNodes.first()'
            -
              type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 6h
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Stale DataNodes'
          type: DEPENDENT
          key: hadoop.namenode.num_stale_data_nodes
          delay: '0'
          history: 7d
          description: 'DataNodes that do not send a heartbeat within 30 seconds are marked as "stale".'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].StaleDataNodes.first()'
            -
              type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 6h
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Block Pool Renaming'
          type: DEPENDENT
          key: hadoop.namenode.percent_block_pool_used
          delay: '0'
          history: 7d
          value_type: FLOAT
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=NameNodeInfo'')].PercentBlockPoolUsed.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Percent capacity remaining'
          type: DEPENDENT
          key: hadoop.namenode.percent_remaining
          delay: '0'
          history: 7d
          value_type: FLOAT
          units: '%'
          description: 'Available capacity in percent.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=NameNodeInfo'')].PercentRemaining.first()'
            -
              type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 6h
          master_item:
            key: hadoop.namenode.get
          triggers:
            -
              expression: '{max(15m)}<{$HADOOP.CAPACITY_REMAINING.MIN.WARN}'
              name: 'NameNode: Cluster capacity remaining is low (below {$HADOOP.CAPACITY_REMAINING.MIN.WARN}% for 15m)'
              priority: WARNING
              description: 'A good practice is to ensure that disk use never exceeds 80 percent capacity.'
        -
          name: 'NameNode: RPC queue & processing time'
          type: DEPENDENT
          key: hadoop.namenode.rpc_processing_time_avg
          delay: '0'
          history: 7d
          value_type: FLOAT
          units: s
          description: 'Average time spent on processing RPC requests.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=RpcActivityForPort9000'')].RpcProcessingTimeAvgTime.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Total load'
          type: DEPENDENT
          key: hadoop.namenode.total_load
          delay: '0'
          history: 7d
          description: 'The current number of concurrent file accesses (read/write) across all DataNodes.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].TotalLoad.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Transactions since last checkpoint'
          type: DEPENDENT
          key: hadoop.namenode.transactions_since_last_checkpoint
          delay: '0'
          history: 7d
          description: 'Total number of transactions since last checkpoint.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].TransactionsSinceLastCheckpoint.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Under-replicated blocks'
          type: DEPENDENT
          key: hadoop.namenode.under_replicated_blocks
          delay: '0'
          history: 7d
          description: 'The number of blocks with insufficient replication.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].UnderReplicatedBlocks.first()'
          master_item:
            key: hadoop.namenode.get
        -
          name: 'NameNode: Uptime'
          type: DEPENDENT
          key: hadoop.namenode.uptime
          delay: '0'
          history: 7d
          value_type: FLOAT
          units: s
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''java.lang:type=Runtime'')].Uptime.first()'
            -
              type: MULTIPLIER
              parameters:
                - '0.001'
          master_item:
            key: hadoop.namenode.get
          triggers:
            -
              expression: '{nodata(30m)}=1'
              name: 'NameNode: Failed to fetch NameNode API page (or no data for 30m)'
              priority: WARNING
              description: 'Zabbix has not received data for items for the last 30 minutes.'
              manual_close: 'YES'
              dependencies:
                -
                  name: 'NameNode: Service is unavailable'
                  expression: '{Hadoop by HTTP:net.tcp.service["tcp","{$HADOOP.NAMENODE.HOST}","{$HADOOP.NAMENODE.PORT}"].last()}=0'
            -
              expression: '{last()}<10m'
              name: 'NameNode: Service has been restarted (uptime < 10m)'
              priority: INFO
              description: 'Uptime is less than 10 minutes'
              manual_close: 'YES'
        -
          name: 'NameNode: Failed volumes'
          type: DEPENDENT
          key: hadoop.namenode.volume_failures_total
          delay: '0'
          history: 7d
          description: 'Number of failed volumes.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=NameNode,name=FSNamesystem'')].VolumeFailuresTotal.first()'
          master_item:
            key: hadoop.namenode.get
          triggers:
            -
              expression: '{min(15m)}>0'
              name: 'NameNode: Cluster has volume failures'
              priority: AVERAGE
              description: 'HDFS now allows for disks to fail in place, without affecting DataNode operations, until a threshold value is reached. This is set on each DataNode via the dfs.datanode.failed.volumes.tolerated property; it defaults to 0, meaning that any volume failure will shut down the DataNode; on a production cluster where DataNodes typically have 6, 8, or 12 disks, setting this parameter to 1 or 2 is typically the best practice.'
        -
          name: 'Get NodeManagers states'
          type: HTTP_AGENT
          key: hadoop.nodemanagers.get
          history: 0h
          trends: '0'
          value_type: TEXT
          applications:
            -
              name: 'Zabbix raw items'
          preprocessing:
            -
              type: JAVASCRIPT
              parameters:
                - 'return JSON.stringify(JSON.parse(JSON.parse(value).beans[0].LiveNodeManagers))'
          url: '{$HADOOP.RESOURCEMANAGER.HOST}:{$HADOOP.RESOURCEMANAGER.PORT}/jmx?qry=Hadoop:service=ResourceManager,name=RMNMInfo'
        -
          name: 'Get ResourceManager stats'
          type: HTTP_AGENT
          key: hadoop.resourcemanager.get
          history: 0h
          trends: '0'
          value_type: TEXT
          applications:
            -
              name: 'Zabbix raw items'
          url: '{$HADOOP.RESOURCEMANAGER.HOST}:{$HADOOP.RESOURCEMANAGER.PORT}/jmx'
        -
          name: 'ResourceManager: Active NMs'
          type: DEPENDENT
          key: hadoop.resourcemanager.num_active_nm
          delay: '0'
          history: 7d
          description: 'Number of Active NodeManagers.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=ResourceManager,name=ClusterMetrics'')].NumActiveNMs.first()'
            -
              type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 6h
          master_item:
            key: hadoop.resourcemanager.get
          triggers:
            -
              expression: '{max(5m)}=0'
              name: 'ResourceManager: Cluster has no active NodeManagers'
              priority: HIGH
              description: 'Cluster is unable to execute any jobs without at least one NodeManager.'
        -
          name: 'ResourceManager: Decommissioned NMs'
          type: DEPENDENT
          key: hadoop.resourcemanager.num_decommissioned_nm
          delay: '0'
          history: 7d
          description: 'Number of Decommissioned NodeManagers.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=ResourceManager,name=ClusterMetrics'')].NumDecommissionedNMs.first()'
          master_item:
            key: hadoop.resourcemanager.get
        -
          name: 'ResourceManager: Decommissioning NMs'
          type: DEPENDENT
          key: hadoop.resourcemanager.num_decommissioning_nm
          delay: '0'
          history: 7d
          description: 'Number of Decommissioning NodeManagers.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=ResourceManager,name=ClusterMetrics'')].NumDecommissioningNMs.first()'
            -
              type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 6h
          master_item:
            key: hadoop.resourcemanager.get
        -
          name: 'ResourceManager: Lost NMs'
          type: DEPENDENT
          key: hadoop.resourcemanager.num_lost_nm
          delay: '0'
          history: 7d
          description: 'Number of Lost NodeManagers.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=ResourceManager,name=ClusterMetrics'')].NumLostNMs.first()'
            -
              type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 6h
          master_item:
            key: hadoop.resourcemanager.get
        -
          name: 'ResourceManager: Rebooted NMs'
          type: DEPENDENT
          key: hadoop.resourcemanager.num_rebooted_nm
          delay: '0'
          history: 7d
          description: 'Number of Rebooted NodeManagers.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=ResourceManager,name=ClusterMetrics'')].NumRebootedNMs.first()'
          master_item:
            key: hadoop.resourcemanager.get
        -
          name: 'ResourceManager: Shutdown NMs'
          type: DEPENDENT
          key: hadoop.resourcemanager.num_shutdown_nm
          delay: '0'
          history: 7d
          description: 'Number of Shutdown NodeManagers.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=ResourceManager,name=ClusterMetrics'')].NumShutdownNMs.first()'
          master_item:
            key: hadoop.resourcemanager.get
        -
          name: 'ResourceManager: Unhealthy NMs'
          type: DEPENDENT
          key: hadoop.resourcemanager.num_unhealthy_nm
          delay: '0'
          history: 7d
          description: 'Number of Unhealthy NodeManagers.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=ResourceManager,name=ClusterMetrics'')].NumUnhealthyNMs.first()'
          master_item:
            key: hadoop.resourcemanager.get
          triggers:
            -
              expression: '{min(15m)}>0'
              name: 'ResourceManager: Cluster has unhealthy NodeManagers'
              priority: AVERAGE
              description: 'YARN considers any node with disk utilization exceeding the value specified under the property yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage (in yarn-site.xml) to be unhealthy. Ample disk space is critical to ensure uninterrupted operation of a Hadoop cluster, and large numbers of unhealthyNodes (the number to alert on depends on the size of your cluster) should be quickly investigated and resolved.'
        -
          name: 'ResourceManager: RPC queue & processing time'
          type: DEPENDENT
          key: hadoop.resourcemanager.rpc_processing_time_avg
          delay: '0'
          history: 7d
          value_type: FLOAT
          units: s
          description: 'Average time spent on processing RPC requests.'
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''Hadoop:service=ResourceManager,name=RpcActivityForPort8031'')].RpcProcessingTimeAvgTime.first()'
          master_item:
            key: hadoop.resourcemanager.get
        -
          name: 'ResourceManager: Uptime'
          type: DEPENDENT
          key: hadoop.resourcemanager.uptime
          delay: '0'
          history: 7d
          value_type: FLOAT
          units: s
          applications:
            -
              name: Hadoop
          preprocessing:
            -
              type: JSONPATH
              parameters:
                - '$.beans[?(@.name==''java.lang:type=Runtime'')].Uptime.first()'
            -
              type: MULTIPLIER
              parameters:
                - '0.001'
          master_item:
            key: hadoop.resourcemanager.get
          triggers:
            -
              expression: '{nodata(30m)}=1'
              name: 'ResourceManager: Failed to fetch ResourceManager API page (or no data for 30m)'
              priority: WARNING
              description: 'Zabbix has not received data for items for the last 30 minutes.'
              manual_close: 'YES'
              dependencies:
                -
                  name: 'ResourceManager: Service is unavailable'
                  expression: '{Hadoop by HTTP:net.tcp.service["tcp","{$HADOOP.RESOURCEMANAGER.HOST}","{$HADOOP.RESOURCEMANAGER.PORT}"].last()}=0'
            -
              expression: '{last()}<10m'
              name: 'ResourceManager: Service has been restarted (uptime < 10m)'
              priority: INFO
              description: 'Uptime is less than 10 minutes'
              manual_close: 'YES'
        -
          name: 'NameNode: Service response time'
          type: SIMPLE
          key: 'net.tcp.service.perf["tcp","{$HADOOP.NAMENODE.HOST}","{$HADOOP.NAMENODE.PORT}"]'
          history: 7d
          value_type: FLOAT
          units: s
          description: 'Hadoop NameNode API performance.'
          applications:
            -
              name: Hadoop
          triggers:
            -
              expression: '{min(5m)}>{$HADOOP.NAMENODE.RESPONSE_TIME.MAX.WARN}'
              name: 'NameNode: Service response time is too high (over {$HADOOP.NAMENODE.RESPONSE_TIME.MAX.WARN} for 5m)'
              priority: WARNING
              manual_close: 'YES'
              dependencies:
                -
                  name: 'NameNode: Service is unavailable'
                  expression: '{Hadoop by HTTP:net.tcp.service["tcp","{$HADOOP.NAMENODE.HOST}","{$HADOOP.NAMENODE.PORT}"].last()}=0'
        -
          name: 'ResourceManager: Service response time'
          type: SIMPLE
          key: 'net.tcp.service.perf["tcp","{$HADOOP.RESOURCEMANAGER.HOST}","{$HADOOP.RESOURCEMANAGER.PORT}"]'
          history: 7d
          value_type: FLOAT
          units: s
          description: 'Hadoop ResourceManager API performance.'
          applications:
            -
              name: Hadoop
          triggers:
            -
              expression: '{min(5m)}>{$HADOOP.RESOURCEMANAGER.RESPONSE_TIME.MAX.WARN}'
              name: 'ResourceManager: Service response time is too high (over {$HADOOP.RESOURCEMANAGER.RESPONSE_TIME.MAX.WARN} for 5m)'
              priority: WARNING
              manual_close: 'YES'
              dependencies:
                -
                  name: 'ResourceManager: Service is unavailable'
                  expression: '{Hadoop by HTTP:net.tcp.service["tcp","{$HADOOP.RESOURCEMANAGER.HOST}","{$HADOOP.RESOURCEMANAGER.PORT}"].last()}=0'
        -
          name: 'NameNode: Service status'
          type: SIMPLE
          key: 'net.tcp.service["tcp","{$HADOOP.NAMENODE.HOST}","{$HADOOP.NAMENODE.PORT}"]'
          history: 7d
          description: 'Hadoop NameNode API port availability.'
          applications:
            -
              name: Hadoop
          valuemap:
            name: 'Service state'
          preprocessing:
            -
              type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 10m
          triggers:
            -
              expression: '{last()}=0'
              name: 'NameNode: Service is unavailable'
              priority: AVERAGE
              manual_close: 'YES'
        -
          name: 'ResourceManager: Service status'
          type: SIMPLE
          key: 'net.tcp.service["tcp","{$HADOOP.RESOURCEMANAGER.HOST}","{$HADOOP.RESOURCEMANAGER.PORT}"]'
          history: 7d
          description: 'Hadoop ResourceManager API port availability.'
          applications:
            -
              name: Hadoop
          valuemap:
            name: 'Service state'
          preprocessing:
            -
              type: DISCARD_UNCHANGED_HEARTBEAT
              parameters:
                - 10m
          triggers:
            -
              expression: '{last()}=0'
              name: 'ResourceManager: Service is unavailable'
              priority: AVERAGE
              manual_close: 'YES'
      discovery_rules:
        -
          name: 'Data node discovery'
          type: HTTP_AGENT
          key: hadoop.datanode.discovery
          delay: 1h
          item_prototypes:
            -
              name: '{#HOSTNAME}: Admin state'
              type: DEPENDENT
              key: 'hadoop.datanode.admin_state[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              trends: '0'
              value_type: CHAR
              description: 'Administrative state.'
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.[?(@.HostName==''{#HOSTNAME}'')].adminState.first()'
                -
                  type: DISCARD_UNCHANGED_HEARTBEAT
                  parameters:
                    - 6h
              master_item:
                key: hadoop.datanodes.get
            -
              name: '{#HOSTNAME}: Used'
              type: DEPENDENT
              key: 'hadoop.datanode.dfs_used[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              units: B
              description: 'Used disk space.'
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''Hadoop:service=DataNode,name=FSDatasetState'')].DfsUsed.first()'
              master_item:
                key: 'hadoop.datanode.get[{#HOSTNAME}]'
            -
              name: 'Hadoop DataNode {#HOSTNAME}: Get stats'
              type: HTTP_AGENT
              key: 'hadoop.datanode.get[{#HOSTNAME}]'
              history: 0h
              trends: '0'
              value_type: TEXT
              applications:
                -
                  name: 'Zabbix raw items'
              url: '{#INFOADDR}/jmx'
            -
              name: '{#HOSTNAME}: JVM Garbage collection time'
              type: DEPENDENT
              key: 'hadoop.datanode.jvm.gc_time[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              units: '!ms'
              description: 'The JVM garbage collection time in milliseconds.'
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''Hadoop:service=DataNode,name=JvmMetrics'')].GcTimeMillis.first()'
              master_item:
                key: 'hadoop.datanode.get[{#HOSTNAME}]'
            -
              name: '{#HOSTNAME}: JVM Heap usage'
              type: DEPENDENT
              key: 'hadoop.datanode.jvm.mem_heap_used[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              value_type: FLOAT
              units: '!MB'
              description: 'The JVM heap usage in MBytes.'
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''Hadoop:service=DataNode,name=JvmMetrics'')].MemHeapUsedM.first()'
              master_item:
                key: 'hadoop.datanode.get[{#HOSTNAME}]'
            -
              name: '{#HOSTNAME}: JVM Threads'
              type: DEPENDENT
              key: 'hadoop.datanode.jvm.threads[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              description: 'The number of JVM threads.'
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''java.lang:type=Threading'')].ThreadCount.first()'
              master_item:
                key: 'hadoop.datanode.get[{#HOSTNAME}]'
            -
              name: '{#HOSTNAME}: Number of failed volumes'
              type: DEPENDENT
              key: 'hadoop.datanode.numfailedvolumes[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              description: 'Number of failed storage volumes.'
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''Hadoop:service=DataNode,name=FSDatasetState'')].NumFailedVolumes.first()'
              master_item:
                key: 'hadoop.datanode.get[{#HOSTNAME}]'
            -
              name: '{#HOSTNAME}: Oper state'
              type: DEPENDENT
              key: 'hadoop.datanode.oper_state[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              trends: '0'
              value_type: CHAR
              description: 'Operational state.'
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.[?(@.HostName==''{#HOSTNAME}'')].operState.first()'
                -
                  type: DISCARD_UNCHANGED_HEARTBEAT
                  parameters:
                    - 6h
              master_item:
                key: hadoop.datanodes.get
              trigger_prototypes:
                -
                  expression: '{last()}<>"Live"'
                  name: '{#HOSTNAME}: DataNode has state {ITEM.VALUE}.'
                  priority: AVERAGE
                  description: 'The state is different from normal.'
            -
              name: '{#HOSTNAME}: Remaining'
              type: DEPENDENT
              key: 'hadoop.datanode.remaining[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              units: B
              description: 'Remaining disk space.'
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''Hadoop:service=DataNode,name=FSDatasetState'')].Remaining.first()'
              master_item:
                key: 'hadoop.datanode.get[{#HOSTNAME}]'
            -
              name: '{#HOSTNAME}: Uptime'
              type: DEPENDENT
              key: 'hadoop.datanode.uptime[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              value_type: FLOAT
              units: s
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''java.lang:type=Runtime'')].Uptime.first()'
                -
                  type: MULTIPLIER
                  parameters:
                    - '0.001'
              master_item:
                key: 'hadoop.datanode.get[{#HOSTNAME}]'
              trigger_prototypes:
                -
                  expression: '{nodata(30m)}=1'
                  name: '{#HOSTNAME}: Failed to fetch DataNode API page (or no data for 30m)'
                  priority: WARNING
                  description: 'Zabbix has not received data for items for the last 30 minutes.'
                  manual_close: 'YES'
                  dependencies:
                    -
                      name: '{#HOSTNAME}: DataNode has state {ITEM.VALUE}.'
                      expression: '{Hadoop by HTTP:hadoop.datanode.oper_state[{#HOSTNAME}].last()}<>"Live"'
                -
                  expression: '{last()}<10m'
                  name: '{#HOSTNAME}: Service has been restarted (uptime < 10m)'
                  priority: INFO
                  description: 'Uptime is less than 10 minutes'
                  manual_close: 'YES'
            -
              name: '{#HOSTNAME}: Version'
              type: DEPENDENT
              key: 'hadoop.datanode.version[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              trends: '0'
              value_type: CHAR
              description: 'DataNode software version.'
              application_prototypes:
                -
                  name: 'Hadoop DataNode {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.[?(@.HostName==''{#HOSTNAME}'')].version.first()'
                -
                  type: DISCARD_UNCHANGED_HEARTBEAT
                  parameters:
                    - 6h
              master_item:
                key: hadoop.datanodes.get
          graph_prototypes:
            -
              name: '{#HOSTNAME}: DataNode {#HOSTNAME} DFS size'
              type: STACKED
              graph_items:
                -
                  drawtype: FILLED_REGION
                  color: 1A7C11
                  item:
                    host: 'Hadoop by HTTP'
                    key: 'hadoop.datanode.dfs_used[{#HOSTNAME}]'
                -
                  sortorder: '1'
                  drawtype: FILLED_REGION
                  color: 2774A4
                  item:
                    host: 'Hadoop by HTTP'
                    key: 'hadoop.datanode.remaining[{#HOSTNAME}]'
          url: '{$HADOOP.NAMENODE.HOST}:{$HADOOP.NAMENODE.PORT}/jmx?qry=Hadoop:service=NameNode,name=NameNodeInfo'
          preprocessing:
            -
              type: JAVASCRIPT
              parameters:
                - |
                  try{
                    parsed = JSON.parse(value);
                    var result = [];
                  
                    function getNodes(nodes) {
                        Object.keys(nodes).forEach(function (field) {
                            var Node = {};
                            Node['{#HOSTNAME}'] = field || '';
                            Node['{#INFOADDR}'] = nodes[field].infoAddr || '';
                            result.push(Node);
                        });
                    }
                  
                    getNodes(JSON.parse(parsed.beans[0].LiveNodes));
                    getNodes(JSON.parse(parsed.beans[0].DeadNodes));
                    getNodes(JSON.parse(parsed.beans[0].DecomNodes));
                    getNodes(JSON.parse(parsed.beans[0].EnteringMaintenanceNodes));
                  
                    return JSON.stringify(result);
                  }
                  catch (error) {
                    throw 'Failed to process response received from Hadoop.';
                  }
                  
        -
          name: 'Node manager discovery'
          type: HTTP_AGENT
          key: hadoop.nodemanager.discovery
          delay: 1h
          item_prototypes:
            -
              name: '{#HOSTNAME}: Available memory'
              type: DEPENDENT
              key: 'hadoop.nodemanager.availablememory[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              units: '!MB'
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$[?(@.HostName==''{#HOSTNAME}'')].AvailableMemoryMB.first()'
              master_item:
                key: hadoop.nodemanagers.get
            -
              name: '{#HOSTNAME}: Container launch avg duration'
              type: DEPENDENT
              key: 'hadoop.nodemanager.container_launch_duration_avg[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              value_type: FLOAT
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''Hadoop:service=NodeManager,name=NodeManagerMetrics'')].ContainerLaunchDurationAvgTime.first()'
              master_item:
                key: 'hadoop.nodemanager.get[{#HOSTNAME}]'
            -
              name: 'Hadoop NodeManager {#HOSTNAME}: Get stats'
              type: HTTP_AGENT
              key: 'hadoop.nodemanager.get[{#HOSTNAME}]'
              history: 0h
              trends: '0'
              value_type: TEXT
              applications:
                -
                  name: 'Zabbix raw items'
              url: '{#NODEHTTPADDRESS}/jmx'
            -
              name: '{#HOSTNAME}: JVM Garbage collection time'
              type: DEPENDENT
              key: 'hadoop.nodemanager.jvm.gc_time[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              units: '!ms'
              description: 'The JVM garbage collection time in milliseconds.'
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''Hadoop:service=NodeManager,name=JvmMetrics'')].GcTimeMillis.first()'
              master_item:
                key: 'hadoop.nodemanager.get[{#HOSTNAME}]'
            -
              name: '{#HOSTNAME}: JVM Heap usage'
              type: DEPENDENT
              key: 'hadoop.nodemanager.jvm.mem_heap_used[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              value_type: FLOAT
              units: '!MB'
              description: 'The JVM heap usage in MBytes.'
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''Hadoop:service=NodeManager,name=JvmMetrics'')].MemHeapUsedM.first()'
              master_item:
                key: 'hadoop.nodemanager.get[{#HOSTNAME}]'
            -
              name: '{#HOSTNAME}: JVM Threads'
              type: DEPENDENT
              key: 'hadoop.nodemanager.jvm.threads[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              description: 'The number of JVM threads.'
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''java.lang:type=Threading'')].ThreadCount.first()'
              master_item:
                key: 'hadoop.nodemanager.get[{#HOSTNAME}]'
            -
              name: '{#HOSTNAME}: Number of containers'
              type: DEPENDENT
              key: 'hadoop.nodemanager.numcontainers[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              trends: '0'
              value_type: CHAR
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$[?(@.HostName==''{#HOSTNAME}'')].NumContainers.first()'
              master_item:
                key: hadoop.nodemanagers.get
            -
              name: '{#HOSTNAME}: RPC queue & processing time'
              type: DEPENDENT
              key: 'hadoop.nodemanager.rpc_processing_time_avg[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              value_type: FLOAT
              description: 'Average time spent on processing RPC requests.'
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''Hadoop:service=NodeManager,name=RpcActivityForPort8040'')].RpcProcessingTimeAvgTime.first()'
              master_item:
                key: 'hadoop.nodemanager.get[{#HOSTNAME}]'
            -
              name: '{#HOSTNAME}: State'
              type: DEPENDENT
              key: 'hadoop.nodemanager.state[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              trends: '0'
              value_type: CHAR
              description: 'State of the node - valid values are: NEW, RUNNING, UNHEALTHY, DECOMMISSIONING, DECOMMISSIONED, LOST, REBOOTED, SHUTDOWN.'
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$[?(@.HostName==''{#HOSTNAME}'')].State.first()'
                -
                  type: DISCARD_UNCHANGED_HEARTBEAT
                  parameters:
                    - 6h
              master_item:
                key: hadoop.nodemanagers.get
              trigger_prototypes:
                -
                  expression: '{last()}<>"RUNNING"'
                  name: '{#HOSTNAME}: NodeManager has state {ITEM.VALUE}.'
                  priority: AVERAGE
                  description: 'The state is different from normal.'
            -
              name: '{#HOSTNAME}: Uptime'
              type: DEPENDENT
              key: 'hadoop.nodemanager.uptime[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              value_type: FLOAT
              units: s
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$.beans[?(@.name==''java.lang:type=Runtime'')].Uptime.first()'
                -
                  type: MULTIPLIER
                  parameters:
                    - '0.001'
              master_item:
                key: 'hadoop.nodemanager.get[{#HOSTNAME}]'
              trigger_prototypes:
                -
                  expression: '{nodata(30m)}=1'
                  name: '{#HOSTNAME}: Failed to fetch NodeManager API page (or no data for 30m)'
                  priority: WARNING
                  description: 'Zabbix has not received data for items for the last 30 minutes.'
                  manual_close: 'YES'
                  dependencies:
                    -
                      name: '{#HOSTNAME}: NodeManager has state {ITEM.VALUE}.'
                      expression: '{Hadoop by HTTP:hadoop.nodemanager.state[{#HOSTNAME}].last()}<>"RUNNING"'
                -
                  expression: '{last()}<10m'
                  name: '{#HOSTNAME}: Service has been restarted (uptime < 10m)'
                  priority: INFO
                  description: 'Uptime is less than 10 minutes'
                  manual_close: 'YES'
            -
              name: '{#HOSTNAME}: Used memory'
              type: DEPENDENT
              key: 'hadoop.nodemanager.usedmemory[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              units: '!MB'
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$[?(@.HostName==''{#HOSTNAME}'')].UsedMemoryMB.first()'
              master_item:
                key: hadoop.nodemanagers.get
            -
              name: '{#HOSTNAME}: Version'
              type: DEPENDENT
              key: 'hadoop.nodemanager.version[{#HOSTNAME}]'
              delay: '0'
              history: 7d
              trends: '0'
              value_type: CHAR
              application_prototypes:
                -
                  name: 'Hadoop NodeManager {#HOSTNAME}'
              preprocessing:
                -
                  type: JSONPATH
                  parameters:
                    - '$[?(@.HostName==''{#HOSTNAME}'')].NodeManagerVersion.first()'
                -
                  type: DISCARD_UNCHANGED_HEARTBEAT
                  parameters:
                    - 6h
              master_item:
                key: hadoop.nodemanagers.get
          url: '{$HADOOP.RESOURCEMANAGER.HOST}:{$HADOOP.RESOURCEMANAGER.PORT}/jmx?qry=Hadoop:service=ResourceManager,name=RMNMInfo'
          preprocessing:
            -
              type: JAVASCRIPT
              parameters:
                - |
                  try {
                    parsed = JSON.parse(value);
                    var result = [];
                  
                    function getNodes(nodes) {
                        Object.keys(nodes).forEach(function (field) {
                            var Node = {};
                            Node['{#HOSTNAME}'] = nodes[field].HostName || '';
                            Node['{#NODEHTTPADDRESS}'] = nodes[field].NodeHTTPAddress || '';
                            result.push(Node);
                        });
                    }
                  
                    getNodes(JSON.parse(parsed.beans[0].LiveNodeManagers));
                  
                    return JSON.stringify(result);
                  }
                  catch (error) {
                    throw 'Failed to process response received from Hadoop.';
                  }
                  
      macros:
        -
          macro: '{$HADOOP.CAPACITY_REMAINING.MIN.WARN}'
          value: '20'
          description: 'The Hadoop cluster capacity remaining percent for trigger expression.'
        -
          macro: '{$HADOOP.NAMENODE.HOST}'
          value: NameNode
          description: 'The Hadoop NameNode host IP address or FQDN.'
        -
          macro: '{$HADOOP.NAMENODE.PORT}'
          value: '9870'
          description: 'The Hadoop NameNode Web-UI port.'
        -
          macro: '{$HADOOP.NAMENODE.RESPONSE_TIME.MAX.WARN}'
          value: 10s
          description: 'The Hadoop NameNode API page maximum response time in seconds for trigger expression.'
        -
          macro: '{$HADOOP.RESOURCEMANAGER.HOST}'
          value: ResourceManager
          description: 'The Hadoop ResourceManager host IP address or FQDN.'
        -
          macro: '{$HADOOP.RESOURCEMANAGER.PORT}'
          value: '8088'
          description: 'The Hadoop ResourceManager Web-UI port.'
        -
          macro: '{$HADOOP.RESOURCEMANAGER.RESPONSE_TIME.MAX.WARN}'
          value: 10s
          description: 'The Hadoop ResourceManager API page maximum response time in seconds for trigger expression.'
  value_maps:
    -
      name: 'Service state'
      mappings:
        -
          value: '0'
          newvalue: Down
        -
          value: '1'
          newvalue: Up
